{"paragraphs":[{"text":"%md\n# Print word frequencies\n\n- After combining the values (counts) with the same key (word), you'll print the word frequencies using the `take(N)` action. You could have used the `collect()` action but as a best practice, it is not recommended as `collect()` returns all the elements from your RDD. You'll use `take(N)` instead, to return N elements from your RDD.\n\n- What if we want to return the top 10 words? For this first, you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count) and print the top 10 words in descending order.\n\n- You already have a `SparkContext` `sc` and `resultRDD` available in your workspace.\n\n## Instructions\n- Print the first 10 words and their frequencies from the `resultRDD`.\n- Swap the keys and values in the `resultRDD`.\n- Sort the keys according to descending order.\n- Print the top 10 most frequent words and their frequencies.","user":"anonymous","dateUpdated":"2021-02-26T16:44:03+0530","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Print word frequencies</h1>\n<ul>\n  <li>\n  <p>After combining the values (counts) with the same key (word), you&rsquo;ll print the word frequencies using the <code>take(N)</code> action. You could have used the <code>collect()</code> action but as a best practice, it is not recommended as <code>collect()</code> returns all the elements from your RDD. You&rsquo;ll use <code>take(N)</code> instead, to return N elements from your RDD.</p></li>\n  <li>\n  <p>What if we want to return the top 10 words? For this first, you&rsquo;ll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you&rsquo;ll sort the pair RDD based on the key (count) and print the top 10 words in descending order.</p></li>\n  <li>\n  <p>You already have a <code>SparkContext</code> <code>sc</code> and <code>resultRDD</code> available in your workspace.</p></li>\n</ul>\n<h2>Instructions</h2>\n<ul>\n  <li>Print the first 10 words and their frequencies from the <code>resultRDD</code>.</li>\n  <li>Swap the keys and values in the <code>resultRDD</code>.</li>\n  <li>Sort the keys according to descending order.</li>\n  <li>Print the top 10 most frequent words and their frequencies.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1614333142789_-71755688","id":"20201113-110338_1928025495","dateCreated":"2021-02-26T15:22:22+0530","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12796","dateFinished":"2021-02-26T16:44:04+0530","dateStarted":"2021-02-26T16:44:03+0530"},{"text":"%pyspark\nstop_words = ['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n 'her',\n 'hers',\n 'herself',\n 'it',\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 'can',\n 'will',\n 'just',\n 'don',\n 'should',\n 'now']\n","user":"anonymous","dateUpdated":"2021-02-26T15:22:22+0530","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1614333142795_1781391256","id":"20201113-192344_1808316642","dateCreated":"2021-02-26T15:22:22+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12797"},{"text":"%pyspark\n\nfile_path = \"file:////home/talentum/shared_BDVM/spark/Complete_Shakespeare.txt\"\n\n# Create a baseRDD from the file path\nbaseRDD = sc.textFile(file_path)\n\n# Split the lines of baseRDD into words\nsplitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n\n# Convert the words in lower case and remove stop words from stop_words\nsplitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n\n# Create a tuple of the word and 1 \nsplitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n\n# Count of the number of occurences of each word\nresultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n\n# Display the first 10 words and their frequencies\nfor word in resultRDD.____(10):\n    print(word)\n\n# Swap the keys and values \nresultRDD_swap = resultRDD.____(lambda x: (x[____], x[____]))\n\n# Sort the keys in descending order\nresultRDD_swap_sort = resultRDD_swap.____(ascending=False)\n\n# Show the top 10 most frequent words and their frequencies\nfor word in resultRDD_swap_sort.____(____):\n    print(\"{} has {} counts\". format(____, word[0]))","user":"anonymous","dateUpdated":"2021-02-26T15:46:19+0530","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1614333142797_-1327081419","id":"20201113-133413_165094595","dateCreated":"2021-02-26T15:22:22+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12798"},{"text":"%pyspark\n\nfile_path = \"file:////home/talentum/shared_BDVM/spark/Complete_Shakespeare.txt\"\n\n# Create a baseRDD from the file path\nbaseRDD = sc.textFile(file_path)\n\n# Split the lines of baseRDD into words\nsplitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n\n# Convert the words in lower case and remove stop words from stop_words\nsplitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n\n# Create a tuple of the word and 1 \nsplitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n\n# Count of the number of occurences of each word\nresultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n\nfor word in resultRDD.take(10):\n    print(word)\n\nresultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n\nresultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n\nfor word in resultRDD_swap_sort.take(10):\n    print(\"{} has {} counts\". format(word[1], word[0]))","user":"anonymous","dateUpdated":"2021-02-26T15:50:13+0530","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('Project', 9)\n('Gutenberg', 7)\n('EBook', 1)\n('Complete', 3)\n('Works', 3)\n('William', 11)\n('Shakespeare,', 1)\n('Shakespeare', 12)\n('', 65498)\n('eBook', 2)\n has 65498 counts\nthou has 650 counts\nthy has 574 counts\nshall has 393 counts\nwould has 311 counts\ngood has 295 counts\nthee has 286 counts\nlove has 273 counts\nEnter has 269 counts\nth' has 254 counts\n"}]},"apps":[],"jobName":"paragraph_1614334603399_-2094756676","id":"20210226-154643_523423565","dateCreated":"2021-02-26T15:46:43+0530","dateStarted":"2021-02-26T15:50:13+0530","dateFinished":"2021-02-26T15:50:14+0530","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12799"},{"text":"%pyspark\n# ","user":"anonymous","dateUpdated":"2021-02-26T15:49:16+0530","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1614334746677_957804353","id":"20210226-154906_560209530","dateCreated":"2021-02-26T15:49:06+0530","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:12800"}],"name":"/BDFWithPySpark/M2/SM4/Ex4","id":"2FXZ27ETN","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}